{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2115c587-b133-44a4-8a30-d02a8340c8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bf0ea8-9bc7-4f5b-a693-05307598e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text reading\n",
    "ProsodyFeatures = pd.read_csv('Features\\\\reading_prosody_features_v1_and_v2_2025.csv')\n",
    "PhonationFeatures = pd.read_csv('Features\\\\reading_phonation_features_v1_and_v2_2025.csv')\n",
    "PhonologicalFeatures = pd.read_csv('Features\\\\reading_phonological_features_v1_and_v2_2025.csv')\n",
    "ArticulationFeatures = pd.read_csv('Features\\\\reading_articulation_features_v1_and_v2_2025.csv')\n",
    "GlottalFeatures = pd.read_csv('Features\\\\glottal_reading_features_v1_and_v2_2025.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cc9e77-b15c-4108-81c2-5b5bef520d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A-vowel phonation\n",
    "ProsodyFeatures = pd.read_csv('Features\\\\a_vowel_phonation_prosody_features_v1_and_v2_2025.csv')\n",
    "PhonationFeatures = pd.read_csv('Features\\\\a_vowel_phonation_phonation_features_v1_and_v2_2025.csv')\n",
    "GlottalFeatures = pd.read_csv('Features\\\\glottal_phonation_features_v1_and_v2_2025.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c8f3a-c3fb-4251-9cd1-3ca982157c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnglishMen = pd.read_csv('Population\\\\EnglishMen-withMT.csv', sep=';')\n",
    "EnglishWomen = pd.read_csv('Population\\\\EnglishWomen-withMT.csv', sep=';')\n",
    "FrenchMen = pd.read_csv('Population\\\\FrenchMen-withMT.csv', sep=';')\n",
    "FrenchWomen = pd.read_csv('Population\\\\FrenchWomen-withMT.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e2862e-28cc-471d-b3dd-19360618eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "population = FrenchWomen.copy()\n",
    "#EnglishMen\n",
    "#EnglishWomen\n",
    "#FrenchMen\n",
    "#FrenchWomen\n",
    "population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00439c33-925c-4c62-a68b-c5a6ae490697",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset = GlottalFeatures.copy()\n",
    "#ProsodyFeatures\n",
    "#PhonationFeatures\n",
    "#ArticulationFeatures\n",
    "#PhonologicalFeatures\n",
    "#GlottalFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182f6e9-7b32-4951-839b-882654cfa9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset = featureset.iloc[:, 1:]\n",
    "featureset['id'] = featureset['id'].str.extract(r'(^\\d+-\\d+)')\n",
    "featureset = featureset[featureset['id'].notna()]\n",
    "# remove kurtosis, skewness, min and max\n",
    "featureset = featureset.drop(columns=featureset.filter(regex='ku|sk|min|max').columns)\n",
    "featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6894863-ed4b-4ba2-a2a6-8f5f2951e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join population and feature set\n",
    "Features = featureset.merge(population, left_on='id', right_on='UniqueId', how='inner')\n",
    "Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0e5ea7-5549-4913-8349-c12b3580a65d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Removing redundant features\n",
    "# This code was written by Bour, C. & Elbeji, A.\n",
    "def calculate_vif(df):\n",
    "    \"\"\"Computes the Variance Inflation Factor (VIF) for each feature in the dataset.\"\"\"\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = df.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "def get_highly_correlated_groups(df, threshold=0.90, alpha=0.05):\n",
    "    \"\"\"Identifies groups of highly correlated features and returns them as a list of sets.\"\"\"\n",
    "    correlation_matrix = df.corr(method='spearman')\n",
    "    p_value_matrix = df.corr(method=lambda x, y: spearmanr(x, y)[1])\n",
    "\n",
    "    correlated_groups = []\n",
    "    visited = set()\n",
    "\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            p_value = p_value_matrix.iloc[i, j]\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold and p_value < alpha:\n",
    "                feature1 = correlation_matrix.columns[i]\n",
    "                feature2 = correlation_matrix.columns[j]\n",
    "\n",
    "                # Find existing group that contains one of these features\n",
    "                found_group = None\n",
    "                for group in correlated_groups:\n",
    "                    if feature1 in group or feature2 in group:\n",
    "                        found_group = group\n",
    "                        break\n",
    "                \n",
    "                if found_group:\n",
    "                    found_group.add(feature1)\n",
    "                    found_group.add(feature2)\n",
    "                else:\n",
    "                    correlated_groups.append({feature1, feature2})\n",
    "\n",
    "    return correlated_groups\n",
    "\n",
    "def iterative_feature_selection(df, correlation_threshold=0.90, vif_threshold=5, alpha=0.05, max_iterations=100):\n",
    "    \"\"\"Iteratively removes highly correlated and high VIF features while keeping at least one.\"\"\"\n",
    "    df_filtered = df.copy()\n",
    "    iteration = 0\n",
    "\n",
    "    while df_filtered.shape[1] > 1 and iteration < max_iterations:\n",
    "        features_before = set(df_filtered.columns)\n",
    "\n",
    "        # Step 1: Identify and remove highly correlated features while keeping one from each group\n",
    "        correlated_groups = get_highly_correlated_groups(df_filtered, correlation_threshold, alpha)\n",
    "        if correlated_groups:\n",
    "            vif_values = calculate_vif(df_filtered).set_index(\"Feature\")\n",
    "\n",
    "            for group in correlated_groups:\n",
    "                if len(group) > 1:\n",
    "                    # Keep the feature with the lowest VIF\n",
    "                    group_vif = vif_values.loc[list(group)]\n",
    "                    feature_to_keep = group_vif[\"VIF\"].idxmin()\n",
    "                    group.remove(feature_to_keep)\n",
    "                    df_filtered = df_filtered.drop(columns=group)\n",
    "\n",
    "        # Step 2: Remove features with high VIF iteratively\n",
    "        vif_values = calculate_vif(df_filtered)\n",
    "        while vif_values[\"VIF\"].max() > vif_threshold:\n",
    "            worst_vif_feature = vif_values.sort_values(by=\"VIF\", ascending=False).iloc[0][\"Feature\"]\n",
    "            df_filtered = df_filtered.drop(columns=[worst_vif_feature])\n",
    "            vif_values = calculate_vif(df_filtered)\n",
    "\n",
    "        # Stop if no features were removed in this iteration\n",
    "        features_after = set(df_filtered.columns)\n",
    "        if features_before == features_after:\n",
    "            break\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    print(f\"\\nðŸ Final feature count: {df_filtered.shape[1]}\")\n",
    "    return df_filtered\n",
    "\n",
    "print(\"----------- Removing features -----------\")\n",
    "features_filtered = Features.drop(\"id\", axis=1).set_index(\"UniqueId\")\n",
    "features_to_select = features_filtered.iloc[:, :-10]\n",
    "selected_features = iterative_feature_selection(features_to_select)\n",
    "Features = selected_features.join(features_filtered.iloc[:, -10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf255424-8c1e-4803-8c87-03eb5af67a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "# The removal of participants with >75% of missing data was carried out at a previous step\n",
    "print(Features[Features.isna().any(axis=1)])\n",
    "print(Features[(Features == 0).sum(axis=1) > 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb08c4-5720-4bc0-88df-6baf2d045113",
   "metadata": {},
   "source": [
    "## Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2973e6c-b078-4ee1-8b01-58f9b559beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = Features.columns[:-10]\n",
    "\n",
    "# Define predictor groups\n",
    "stress_predictor = [\"stress_score\"]\n",
    "base_adjustment = [\"age\", \"educ\", \"alc\", \"smk_1\", \"chronicD\", \"mother-tongue\"]\n",
    "full_adjustment = base_adjustment + [\"who5\", \"phq9\", \"fss\"]\n",
    "\n",
    "# Standardize numerical predictors (except binary/categorical variables)\n",
    "scaler = StandardScaler()\n",
    "scaled_features = Features.copy()\n",
    "scaled_features[[\"stress_score\", \"age\", \"alc\", \"who5\", \"phq9\", \"fss\"]] = scaler.fit_transform(\n",
    "    Features[[\"stress_score\", \"age\", \"alc\", \"who5\", \"phq9\", \"fss\"]]\n",
    ")\n",
    "\n",
    "# Encode categorical variables\n",
    "scaled_features = pd.get_dummies(scaled_features, columns=[\"educ\", \"smk_1\", \"chronicD\", \"mother-tongue\"], drop_first=True)\n",
    "\n",
    "# Update predictor lists to reflect new column names\n",
    "categorical_vars = [\"educ_higher\", \"smk_1_Smoker\", \"chronicD_yes\", \"mother-tongue_other\"]\n",
    "base_adjustment = [\"age\", \"alc\"] + categorical_vars\n",
    "full_adjustment = base_adjustment + [\"who5\", \"phq9\", \"fss\"]\n",
    "\n",
    "# Prepare results storage\n",
    "results = []\n",
    "\n",
    "# Loop through each selected feature\n",
    "for feature in selected_features:\n",
    "    # Check for NaNs and 0s in the feature\n",
    "    if Features[feature].isna().sum() > 0:\n",
    "        print(f\"Warning: {feature} has missing values. Consider handling them.\")\n",
    "    if (Features[feature] == 0).sum() > 0:\n",
    "        print(f\"Warning: {feature} has zero values.\")\n",
    "\n",
    "    # Standardize the dependent variable\n",
    "    y = (scaled_features[feature] - scaled_features[feature].mean()) / scaled_features[feature].std()\n",
    "\n",
    "    # Define models\n",
    "    models = [\n",
    "        (\"Model 1\", stress_predictor),\n",
    "        (\"Model 2\", stress_predictor + base_adjustment),\n",
    "        (\"Model 3\", stress_predictor + full_adjustment)\n",
    "    ]\n",
    "\n",
    "    # Run regressions\n",
    "    for model_name, predictors in models:\n",
    "        X = scaled_features[predictors].astype(float)\n",
    "        X = sm.add_constant(X)  # Add intercept\n",
    "        \n",
    "        # Fit OLS model\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        \n",
    "        # Store results\n",
    "        ci_values = model.conf_int()\n",
    "        for param, coef, pval in zip(model.params.index, model.params, model.pvalues):\n",
    "                if param == \"const\":\n",
    "                    continue  # skip intercept\n",
    "                results.append({\n",
    "                    \"Feature\": feature,\n",
    "                    \"Model\": model_name,\n",
    "                    \"Predictor\": param,\n",
    "                    \"Coefficient\": coef,\n",
    "                    \"CI Lower\": ci_values.loc[param, 0],\n",
    "                    \"CI Upper\": ci_values.loc[param, 1],\n",
    "                    \"p-value\": pval\n",
    "                })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Create a new column for FDR-corrected p-values, defaulting to NaN\n",
    "results_df[\"FDR-adjusted p\"] = np.nan\n",
    "\n",
    "# Apply FDR correction within each model for stress_score only\n",
    "for model in results_df[\"Model\"].unique():\n",
    "    mask = (results_df[\"Model\"] == model) & (results_df[\"Predictor\"] == \"stress_score\")\n",
    "    pvals = results_df.loc[mask, \"p-value\"]\n",
    "    if not pvals.empty:\n",
    "        corrected = multipletests(pvals, method=\"fdr_bh\")[1]\n",
    "        results_df.loc[mask, \"FDR-adjusted p\"] = corrected\n",
    "\n",
    "# Show final results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff866faa-5954-42db-985d-037e3377ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('resultats-disvoice.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60372c9-61fa-4a46-9b53-7c940dd8bc57",
   "metadata": {},
   "source": [
    "### Formatting results in Tables S3 and S4 showing the progressive adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe47ce3-7127-4441-a1df-ea23cffcb4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stress = results_df[results_df['Predictor'] == 'stress_score'].copy()\n",
    "\n",
    "# Format the coefficient and confidence interval\n",
    "df_stress['formatted_result'] = df_stress.apply(\n",
    "    lambda row: f\"{row['Coefficient']:.2f} [{row['CI Lower']:.2f}, {row['CI Upper']:.2f}]\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Extract model number (assuming 'Model 1', 'Model 2', etc.)\n",
    "df_stress['Model_Num'] = df_stress['Model'].str.extract(r'(\\d+)')\n",
    "\n",
    "# Pivot to wide format with one row per Feature\n",
    "result = df_stress.pivot(index='Feature', columns='Model_Num', values=['formatted_result', 'FDR-adjusted p'])\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "result.columns = [\n",
    "    f\"M{col[1]}\" if col[0] == 'formatted_result' else f\"pvalue{col[1]}\"\n",
    "    for col in result.columns\n",
    "]\n",
    "\n",
    "# Optional: sort columns to desired order\n",
    "ordered_cols = []\n",
    "for i in range(1, 4):  # for Model 1 to Model 3\n",
    "    if f\"M{i}\" in result.columns and f\"pvalue{i}\" in result.columns:\n",
    "        ordered_cols.extend([f\"M{i}\", f\"pvalue{i}\"])\n",
    "result = result[ordered_cols]\n",
    "\n",
    "# Reset index to bring Feature back as a column\n",
    "result = result.reset_index()\n",
    "\n",
    "# Display or return result\n",
    "print(result)\n",
    "result.to_csv('resultats-disvoice-progressive.csv', sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
